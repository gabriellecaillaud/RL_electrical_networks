{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Système multi-agents pour reprise d'installations électiques"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "L'objectif final est de concevoir une méthodologie pour former une stratégie de réparation du système de distribution HTA en situation de crise."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Techniquement, la conception est divisée en trois parties complémentaires :\n",
    "\n",
    "    1 - Simuler l'environnement du système de distribution HTA -Anylogic- \n",
    "    2 - Sélectionner l'algorithme d'apprentissage par renforcement le plus pertinent pour ce type de problème -PPO- \n",
    "    3 - Construire un lien entre le simulateur et l'algorithme d'entraînement RL.\n",
    "\n",
    "Ensuite, nous lançons l'entraînement du modèle RL sur le simulateur d'environnement et enfin, nous testons notre simulation avec le modèle entraîné.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Remarque :\n",
    "    \n",
    "    - Créez un nouveau environnement et installer tous les packages nécessaires\n",
    "    - N'oubliez pas de changer les chemin d'accès dans le code."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implémentation de PPO :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Proximal Policy Optimization (PPO) est utilisée dans plusieurs solutions pour entraîner un modèle de décision sur un environnement industriel. PPO permet d'entraîner un modèle sur un espace d'actions discrètes ou continues. Il est plus rapide à converger et surpasse A2C et DDPG avec de bonnes performances.\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Article de \"Towards Data Science\":\n",
    "The idea is that PPO improves the stability of the Actor training by limiting the policy update at each training step.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "import numpy as np\n",
    "# on utilise Pytorch pour construire et entrainer notre modele PPO\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Les observations sur lesquelles on travaille \n",
    "\n",
    "[etatequipement=[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0], disponibleequipe=[1, 1, 1], emplequipe=[50, 50, 50], recompns=15500.0]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Les actions à prendre doivent être sous la forme \n",
    "\n",
    "a.req1 = 2  # l'equipe 1 vers l'équipement 1\n",
    "\n",
    "a.req2 = 6 # l'equipe 2 vers l'équipement 2\n",
    "\n",
    "a.req3 = 48 # l\"equipe 3 vers l'equipement 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import alpyne\n",
    "from alpyne.client import alpyne_client\n",
    "from alpyne.client.alpyne_client import AlpyneClient\n",
    "from alpyne.data.spaces import Action"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "    IMPORTANT: Il faut mettre l'emplacement du modèle exporté de Anylogic.\n",
    "    \n",
    "    le modèle de simulation doit utiliser la classe RLExperiment pour définir l'espace d'action, d'observation et de configuration initiale.\n",
    "\n",
    "    Le modèle anylogic doit contenir un événement cyclique (qui se déclenche à chaque période de temps) avec la méthode RLtakeAction()."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "obs=sim.get_observation()\n",
    "print(obs)\n",
    "#print(type(obs))\n",
    "#éol=obs.etatequipement\n",
    "#print(éol)\n",
    "#print(type(equip))\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sim' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m obs\u001b[38;5;241m=\u001b[39m\u001b[43msim\u001b[49m\u001b[38;5;241m.\u001b[39mget_observation()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(obs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sim' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# initialisation d'une couche de Réseau de Neurones - Neural Network 'NN'\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    #Transforme en matrice orthogonale\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class agent_PPO(nn.Module):\n",
    "    '''\n",
    "    Attention ! Il reste des 'len' à modifier par des .size si jamais on les met sous forme de tenseur pytorch\n",
    "    '''\n",
    "\n",
    "    def __init__(self,obs0):\n",
    "       #on considère que obs0 est l'objet alpyne des observations initiales\n",
    "       #super().__init__ permet d'indiquer que agent_PPO est une sous-classe de nn.Module \n",
    "        super(agent_PPO,self).__init__()\n",
    "\n",
    "        self.obs0=obs0\n",
    "        self.n_equipe=len(obs0.disponibleequipe)\n",
    "        self.n_equipement=len(obs0.etatequipement)\n",
    "\n",
    "        #std = variance\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_equipement, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0),\n",
    "            \n",
    "        )\n",
    "        # NN de actor (pour la politique)\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(self.n_equipement, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, self.n_equipement*self.n_equipe), std=0.01))\n",
    "    \n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        #Les logits sont les probabilités de chaque action \n",
    "        logits = self.actor(x)\n",
    "        if logits.nelement()>self.n_equipe*self.n_equipement:\n",
    "            logits=logits.reshape([x.size(0),self.n_equipe,self.n_equipement])\n",
    "        else:\n",
    "            logits=logits.reshape([self.n_equipe,self.n_equipement])\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "x=torch.tensor([[[1,0.2,3],[2,3,4]],[[3,4,1],[5,6,1]]])\n",
    "x2=torch.tensor([1,0.2,3,2,3,4,3,4,1,5,6,1])\n",
    "x3=torch.tensor([0.2,0.4,6])\n",
    "\n",
    "print(x.size(0))\n",
    "y=x2.reshape([x.size(0),2,3])\n",
    "x.nele\n",
    "print(y)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2\n",
      "tensor([[[1.0000, 0.2000, 3.0000],\n",
      "         [2.0000, 3.0000, 4.0000]],\n",
      "\n",
      "        [[3.0000, 4.0000, 1.0000],\n",
      "         [5.0000, 6.0000, 1.0000]]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "n_equipe=3\n",
    "n_equipement=50\n",
    "logits=torch.rand(n_equipe*n_equipement)\n",
    "print(logits.size())\n",
    "logits=logits.reshape([3,50])\n",
    "print(logits.size())\n",
    "probs = Categorical(logits=logits)\n",
    "action = probs.sample()\n",
    "print(probs.log_prob(action))\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([150])\n",
      "torch.Size([3, 50])\n",
      "tensor([-3.6101, -3.5542, -3.8945])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "x=torch.tensor([[1,2],[3,4]])\n",
    "print(x.sum(dim=-1))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([3, 7])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Les paramètres "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Paramètres environnement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# seed des expériences\n",
    "seed =1\n",
    "\n",
    "# Le nombre des environnements parallèles\n",
    "num_envs = 1\n",
    "# Le nombre de pas à exécuter dans un seul environnement pendant la phase policy rollout (128)\n",
    "num_steps =128"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Paramètres d'entrainements "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Le nombre total des pas-de-temps des expériences\n",
    "total_timesteps = 500\n",
    "\n",
    "gamma=0.95 #Discount qui prend en compte jusqu'à 100 étapes dans le calcul de la reward \n",
    "\n",
    "\n",
    "# Pour réduire le pas d'apprentissage(learning_rate) des NN, politique(Pi) et valeur (VF) \"\n",
    "anneal_lr = True\n",
    "\n",
    "learning_rate = 2.5e-4 #Pas d'apprentissage de l'optimizer, on pourra essayer quelque chose de l'ordre de 10^-3 par la suite \n",
    "\n",
    "batch_size = int(num_envs * num_steps)\n",
    "num_minibatches = 4\n",
    "\n",
    "minibatch_size = int(batch_size // num_minibatches)\n",
    "\n",
    "epochs=10\n",
    "\n",
    "epsilon_greedy=0.1 #Probabilité d'explorer initiale \n",
    "\n",
    "# Lambda pour GAE\n",
    "gae_lambda = 0.95\n",
    "\n",
    "# Utilisation de GAE : General Advantage Estimation \n",
    "gae = True\n",
    "\n",
    "# le seuil de KL divergence\n",
    "target_kl = None\n",
    "\n",
    "# permmettre la normalisation de l'advantages (soustraire la moyenne / diviser par l'écart type)\n",
    "norm_adv = True\n",
    "\n",
    "# le coefficient de clipping PPO (epsilon)\n",
    "clip_coef = 0.2\n",
    "# Permettre l'utilisation de clipped loss dans la fonction valeur\n",
    "clip_vloss = True\n",
    "\n",
    "#Coefficient d'entropie. Dans le PPO un coefficient d’entropie est multiplié à l’entropie maximale possible \n",
    "# et ajouté à la fonction objectif. Ceci contribue à la régularisation en prévenant une convergence prématurée \n",
    "# vers une action de probabilité élevée. Entropie élevée : actions équiprobables\n",
    "\n",
    "ent_coef = 0.01\n",
    "# coefficient de la fonction valeur dans la fonction obj\n",
    "vf_coef = 0.5\n",
    "# la norme maximale pour le gradient clipping\n",
    "max_grad_norm = 0.5\n",
    "\n",
    "# les K epochs pour mise à jour la politique\n",
    "update_epochs = 4"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Cette instance est créée pour lancer l'environnement de simulation.\n",
    "# Pour lancer la simulation le modèle de simulation anylogic doit etre fini avec la classe RLExperiment et exporté.\n",
    "# Il faut mettre l'emplacement du modèle exporté de Anylogic.\n",
    "client = AlpyneClient(r\"/Volumes/Stock/IA/Code/Anylogic/RL_Anylogic/Exported/model.jar\", verbose = True, port = 51150)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-12-08 09:19:08,510 [alpyne.client.alpyne_client @ 69][    INFO] Starting model in /Volumes/Stock/IA/Code/Anylogic/RL_Anylogic/Exported\n",
      "2022-12-08 09:19:08,913 [alpyne.client.alpyne_client @ 89][   DEBUG] Executing:\n",
      "java -cp ../alpyne/resources/*:../alpyne/resources/alpyne_lib/*:*:cache/*:database/*:lib/*:lib/database/*:lib/ecj/*:lib/gis/*:lib/poi/*:lib/sa/*:lib/database/querydsl/*:lib/sa/jackson/*:lib/sa/spark/* com.anylogic.alpyne.AlpyneServer -p 51150 -o /Volumes/Stock/IA/Code/Anylogic/RL_Anylogic -l ALL .\n",
      "\n",
      "2022-12-08 09:19:08,943 [alpyne.client.alpyne_client @ 107][    INFO] Started app | PID = 1321\n",
      "2022-12-08 09:19:08,946 [alpyne.client.http_client @ 31][   DEBUG] GET /versions/number/0: None\n",
      "2022-12-08 09:19:17,771 [alpyne.client.http_client @ 64][   DEBUG] => 200 (OK) [('Content-Type', 'application/json'), ('Connection', 'close'), ('Content-Length', '1763')] {'version': 0, 'experimentTemplate': {'outputs': [{'name': 'statsEqWorking', 'type': 'STATISTICS_CONTINUOUS', 'value': None, 'units': None}, {'name': 'gain', 'type': 'DOUBLE'}], 'reinforcement_learning': {'action': [{'name': 'req1', 'type': 'INTEGER'}, {'name': 'req2', 'type': 'INTEGER'}, {'name': 'req3', 'type': 'INTEGER'}], 'configuration': [{'name': 'nbequipe', 'type': 'INTEGER'}, {'name': '{START_TIME}', 'type': 'DOUBLE', 'value': 0.0, 'units': 'HOUR'}, {'name': '{START_DATE}', 'type': 'DATE_TIME', 'value': '2019-12-31T23:00:00Z', 'units': None}, {'name': '{STOP_TIME}', 'type': 'DOUBLE', 'value': 'Infinity', 'units': 'HOUR'}, {'name': '{STOP_DATE}', 'type': 'DATE_TIME', 'value': None, 'units': None}, {'name': '{RANDOM_SEED}', 'type': 'LONG'}], 'observation': [{'name': 'etatequipement', 'type': 'INTEGER_ARRAY'}, {'name': 'disponibleequipe', 'type': 'INTEGER_ARRAY'}, {'name': 'emplequipe', 'type': 'INTEGER_ARRAY'}, {'name': 'recompns', 'type': 'DOUBLE'}]}, 'inputs': [{'name': 'DailyRevenuePerUnit', 'type': 'DOUBLE'}, {'name': 'ReplacementCost', 'type': 'DOUBLE'}, {'name': 'RepairCost', 'type': 'DOUBLE'}, {'name': 'MaintenanceCost', 'type': 'DOUBLE'}, {'name': 'ServiceCrewCostPerDay', 'type': 'DOUBLE'}, {'name': 'ReplaceOldEquipment', 'type': 'BOOLEAN'}, {'name': 'MtcePeriodsToReplace', 'type': 'INTEGER'}, {'name': 'ServiceCapacity', 'type': 'INTEGER'}, {'name': 'p1', 'type': 'INTEGER'}, {'name': 'p2', 'type': 'INTEGER'}, {'name': 'p3', 'type': 'INTEGER'}, {'name': 'inRLMode', 'type': 'BOOLEAN'}, {'name': 'inRLTestMode', 'type': 'BOOLEAN'}, {'name': '{START_TIME}', 'type': 'DOUBLE', 'value': 0.0, 'units': 'HOUR'}, {'name': '{START_DATE}', 'type': 'DATE_TIME', 'value': '2019-12-31T23:00:00Z', 'units': None}, {'name': '{STOP_TIME}', 'type': 'DOUBLE', 'value': 'Infinity', 'units': 'HOUR'}, {'name': '{STOP_DATE}', 'type': 'DATE_TIME', 'value': None, 'units': None}, {'name': '{RANDOM_SEED}', 'type': 'LONG'}]}}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# initialisation de la configuration du simulateur \n",
    "cfg = client.configuration_template\n",
    "cfg.nbequipe = 3\n",
    "cfg.engine_seed = 1  "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# lancement d'une simulation avec la configuration initiale\n",
    "sim = client.create_reinforcement_learning(cfg)\n",
    "sim=sim.run()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-12-08 09:19:22,177 [alpyne.client.http_client @ 31][   DEBUG] POST /runs: {'experimentType': <ExperimentType.REINFORCEMENT_LEARNING: 'REINFORCEMENT_LEARNING'>, 'inputs': [nbequipe:INTEGER=3, {START_TIME}:DOUBLE=0 HOUR, {START_DATE}:DATE_TIME=2019-12-31T23:00:00Z, {STOP_TIME}:DOUBLE=inf HOUR, {STOP_DATE}:DATE_TIME=None, {RANDOM_SEED}:LONG=1]}\n",
      "2022-12-08 09:19:22,507 [alpyne.client.http_client @ 64][   DEBUG] => 201 (Created) [('Location', 'http://127.0.0.1:51150/runs/1'), ('Content-Type', 'application/json'), ('Connection', 'close'), ('Content-Length', '147')] {'status': 'RUNNING', 'id': 1, 'message': '{\"reason\":null,\"next_event_time\":\"-Infinity\",\"episode_count\":1,\"step_count\":0,\"model_time\":0.0}'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "obs=sim.get_observation()\n",
    "agent=agent_PPO(obs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-11-22 16:52:30,302 [alpyne.client.http_client @ 31][   DEBUG] POST /runs/1/rl: {'command': 'OBSERVATION'}\n",
      "2022-11-22 16:52:30,412 [alpyne.client.http_client @ 64][   DEBUG] => 200 (OK) [('Content-Type', 'application/json'), ('Connection', 'close'), ('Content-Length', '339')] [{'name': 'etatequipement', 'type': 'INTEGER_ARRAY', 'value': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]}, {'name': 'disponibleequipe', 'type': 'INTEGER_ARRAY', 'value': [1, 1, 1]}, {'name': 'emplequipe', 'type': 'INTEGER_ARRAY', 'value': [50, 50, 50]}, {'name': 'recompns', 'type': 'DOUBLE', 'value': 0.0}]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sim=sim.reset(inputs=[{'name': 'etatequipement', 'type': 'INTEGER_ARRAY', 'value': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0]}, {'name': 'disponibleequipe', 'type': 'INTEGER_ARRAY', 'value': [1, 1, 1]}, {'name': 'emplequipe', 'type': 'INTEGER_ARRAY', 'value': [50, 50, 50]}, {'name': 'recompns', 'type': 'DOUBLE', 'value': 0.0}])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "print(sim.get_observation())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "2022-11-19 18:46:02,517 [alpyne.client.http_client @ 31][   DEBUG] POST /runs/1/rl: {'command': 'OBSERVATION'}\n",
      "2022-11-19 18:46:02,540 [alpyne.client.http_client @ 64][   DEBUG] => 200 (OK) [('Content-Type', 'application/json'), ('Connection', 'close'), ('Content-Length', '342')] [{'name': 'etatequipement', 'type': 'INTEGER_ARRAY', 'value': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0]}, {'name': 'disponibleequipe', 'type': 'INTEGER_ARRAY', 'value': [1, 0, 1]}, {'name': 'emplequipe', 'type': 'INTEGER_ARRAY', 'value': [30, 44, 6]}, {'name': 'recompns', 'type': 'DOUBLE', 'value': -3000.0}]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[etatequipement=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0], disponibleequipe=[1, 0, 1], emplequipe=[30, 44, 6], recompns=-3000.0]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "global agent\n",
    "L=torch.Tensor([])\n",
    "def train():\n",
    "    # Initialiser l'agent    \n",
    "    optimizer = torch.optim.Adam(agent.parameters(), lr=learning_rate, eps=1e-5)\n",
    "\n",
    "    # Initialiser l'environnement\n",
    "    global sim\n",
    "    global L\n",
    "    #sim = sim.reset()\n",
    "\n",
    "    # Initialisation de l'espace de stockage (buffer) \n",
    "    obs = torch.zeros((num_steps, agent.n_equipement,num_envs)) # Une colonne = 1 environnement différent si on veut upgrade \n",
    "    actions = torch.zeros((num_steps, agent.n_equipe,num_envs))\n",
    "    logprobs = torch.zeros((num_steps,agent.n_equipe, num_envs))\n",
    "    rewards = torch.zeros((num_steps, num_envs))\n",
    "    dones = torch.zeros((num_steps, num_envs))\n",
    "    values = torch.zeros((num_steps, num_envs))\n",
    "\n",
    "    # Commencer l'entrainement\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(sim.get_observation().etatequipement) # APLYNE, attention : fonctionnel pour un seul environnement\n",
    "    next_obs=next_obs[:,None ] #À modifier si jamais il y a plus d'un env \n",
    "    next_done = torch.zeros(num_envs)\n",
    "    num_updates = total_timesteps // batch_size\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # Déminuer le taux d'apprentissage si aneal_lr : vrai\n",
    "        if anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * learning_rate\n",
    "            optimizer.param_groups[0][\"lr\"] = lrnow #Pour changer le learning rate de l'optimizer\n",
    "        #Phase rollout\n",
    "        for step in range(num_steps):\n",
    "            global_step+=1*num_envs\n",
    "            obs[step]=next_obs #Observations sur le nombre total d'étapes (steps) pour chaque environnement, pour l'instant un seul\n",
    "            dones[step]=next_done\n",
    "            with torch.no_grad():\n",
    "                action,log_prob,_,value=agent.get_action_and_value(next_obs.flatten()) #Changer le flatten en reshape pour plusieurs env\n",
    "                values[step]=value.flatten()\n",
    "            \n",
    "            action=action[:,None ] #À modifier si jamais il y a plus d'un env \n",
    "            actions[step]=action\n",
    "\n",
    "            log_prob=log_prob[:,None]\n",
    "            logprobs[step]=log_prob\n",
    "\n",
    "            #Execution des actions\n",
    "            a=client.action_template\n",
    "\n",
    "            # A CHANGER\n",
    "            a.req1=action[0].item()\n",
    "            a.req2=action[1].item()\n",
    "            a.req3=action[2].item()\n",
    "\n",
    "            #\n",
    "            sim.take_action(a)\n",
    "            observ=sim.get_observation()\n",
    "            next_obs,reward,done=observ.etatequipement,observ.recompns,sim.is_terminal()\n",
    "\n",
    "            reward=torch.Tensor([reward]).view(-1)\n",
    "            reward=reward[:,None]\n",
    "\n",
    "            rewards[step]=reward\n",
    "            next_obs,next_done=torch.Tensor(next_obs),torch.Tensor([done])\n",
    "            next_obs=next_obs[:,None ] #À modifier si jamais il y a plus d'un env\n",
    "\n",
    "\n",
    "            print(f'\\r global_step={global_step}, step={step}, reward={reward}')\n",
    "        # Calcul de la fonction Advantage\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs.flatten()).reshape(1, -1)\n",
    "            if gae:\n",
    "                advantages = torch.zeros_like(rewards)\n",
    "                lastgaelam = 0\n",
    "                for t in reversed(range(num_steps)):\n",
    "                    if t == num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        nextvalues = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        nextvalues = values[t + 1]\n",
    "                    delta = rewards[t] + gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    advantages[t] = lastgaelam = delta + gamma * gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "            else:\n",
    "                returns = torch.zeros_like(rewards)\n",
    "                for t in reversed(range(num_steps)):\n",
    "                    if t == num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        next_return = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        next_return = returns[t + 1]\n",
    "                    returns[t] = rewards[t] + gamma * nextnonterminal * next_return\n",
    "                advantages = returns - values\n",
    "\n",
    "        # le batch : avoir un vecteur de tous les éléments utilisés\n",
    "        b_obs = obs.reshape((-1,) + (len(observ.etatequipement),))\n",
    "        b_logprobs = logprobs.reshape((-1,) + (agent.n_equipe,))\n",
    "        b_actions = actions.reshape((-1,) + (len(observ.disponibleequipe),))\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "        #print(\"advantages is\",advantages.size())\n",
    "        #print(\"returns is:\",returns.size())\n",
    "        #print('b obs size', b_obs.size())\n",
    "\n",
    "        \n",
    "\n",
    "        # Optimisation de la fonction valeur et la politique\n",
    "        b_inds = np.arange(batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            # mini-batching\n",
    "            for start in range(0, batch_size, minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "                #print(\"mb_inds size is:\",mb_inds.shape)\n",
    "                #print(\"L'élément perturbateur size :\",b_obs[mb_inds].size())\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "\n",
    "                #print(\"new log prob size :\",newlogprob.size())\n",
    "                #print('b_logprobs size :',b_logprobs[mb_inds].size())\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calcul de approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > clip_coef).float().mean().item()]\n",
    "                    ratio=ratio.sum(dim=-1) # À MODIFIER PLUS TARD\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                # normalisation de l'avantage \n",
    "                if norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "                # Coût associé à la politique\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - clip_coef, 1 + clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Coût associé à la fonction valeur\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -clip_coef,\n",
    "                        clip_coef,\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    loss=torch.tensor([loss.item()])\n",
    "                    L=torch.cat((L,loss))\n",
    "\n",
    "            # early stopping\n",
    "            if target_kl is not None:\n",
    "                if approx_kl > target_kl:\n",
    "                    break\n",
    "        \n",
    "            #print(\"epoch loss is :\",loss)\n",
    "    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "    var_y = np.var(y_true)\n",
    "    end_time=time.time()\n",
    "    print(\"computation time :\",end_time-start_time)\n",
    "        \n",
    "    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "print(L)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([39168536., 32479252., 23933860., 45106832., 29999458., 43155684.,\n",
      "        40442900., 27088794., 47407168., 39997424., 23265914., 30015300.,\n",
      "        29008528., 33437918., 42302484., 35936404., 49460996., 43876544.,\n",
      "        46941744., 44964892., 41507900., 49986580., 49180940., 44565304.,\n",
      "        51579872., 45576344., 42606384., 45477308., 42414560., 44894228.,\n",
      "        47081884., 50849228., 62407844., 64817040., 60078184., 71117744.,\n",
      "        63103188., 68113504., 63325924., 63875972., 58342668., 68347072.,\n",
      "        66502884., 65223240., 68107104., 63713556., 61031724., 65562824.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ALPyne : "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "L=torch.Tensor([])\n",
    "x=torch.tensor([2])\n",
    "L=torch.cat((L,x))\n",
    "print(L)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "ALPyne (AnyLogic-Python connector) est un outil implémenté pour connecter l'environnement du simulateur anylogic avec des algorithmes python qui permettent l'entraînement d'un modèle RL en se basant sur le modèle exporté de Anylogic."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "    Anylogic est un outil de simulation pour les environnements industriels, plus d'infos : https://www.anylogic.com/"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entrainement du modele "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dans cette partie, nous établissons un lien entre l'environnement Anylogic et l'environnement d'entraînement Python en utilisant la bibliothèque Alpyne.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Le rôle d'Alpyne est de permettre l'utilisation de l'environnement développé sur Anylogic comme environnement virtuel pour entraîner un modèle de décision utilisant un algorithme RL.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Dans ce qui suit, un exemple d'entraînement d'un modèle décisionnel PPO sur un environnement de parc éolien est présenté. Le modèle récupère l'observation (l'état actuel de l'environnement) et prend une action sous forme d'envoi d'équipes de réparation, l'action est transmise et exécutée par l'environnement qui change d'état et renvoie une récompense qui définit la qualité de l'action précédente.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "OpenAI développe une bibliothèque de base stable dont le PPO fait partie des algorithmes RL implémentés, le PPO implémenté dans cette bibliothèque est plus optimisé. Le PPO implémenté nécessite l'utilisation d'un environnement compatible avec les normes de la bibliothèque de gymnastique. Afin de répondre à ce besoin, alpyne fournit une classe abstraite \"BaseAlpyneEnv\" qui permet d'implémenter les fonctions d'un environnement de gym.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Il est conseillé de redémarrer l'environnement d'exécution python lors de l'apprentissage d'un nouveau modèle, car dans de nombreux cas, le serveur créé par Alpyne reste actif après la fin de l'exécution."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "### cette fonction retourne une instance de l'environnement gym avec l'id correspondant\n",
    "\"\"\"Cette implmentation pour verifier l'algorithme aec gym  environement\"\"\"\n",
    "def make_env(gym_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(gym_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env.seed=seed\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "# gym pour implémenter l'environnement d'apprentissage\n",
    "from gym import spaces\n",
    "\n",
    "# on utilise les blocs Try:Except: afin de supprimer les commentaires d'installation ou avertassiement.\n",
    "\n",
    "from alpyne.client.alpyne_client import AlpyneClient\n",
    "from alpyne.client.model_run import ModelRun\n",
    "from alpyne.data.spaces import Observation, Action\n",
    "\n",
    "from alpyne.client.abstract import BaseAlpyneEnv\n",
    "\n",
    "# création de l'environnement d'apprentissage avec standards gym à travers l'héritage et implémentation des\n",
    "# fonctions de BaseAlpyneEnv\n",
    "\n",
    "class EolienParc(BaseAlpyneEnv):\n",
    "\n",
    "    def __init__(self, sim: ModelRun):\n",
    "        super().__init__(sim)\n",
    "\n",
    "    # définie l'espace des observation sous forme d'un vecteur avec les valeurs min et max de chaque champs\n",
    "    def _get_observation_space(self) -> spaces.Space:\n",
    "        low1 = np.zeros(50)\n",
    "        high1 = np.ones(50)\n",
    "        low2 = np.zeros(3)\n",
    "        high2 = np.ones(3)\n",
    "        low3 = np.array([0, 0, 0])\n",
    "        high3 = np.array([50, 50, 50])\n",
    "        low4 = np.array([-4500])\n",
    "        high4 = np.array([+4500])\n",
    "        return spaces.Box(low=np.concatenate((low1, low2, low3, low4)), high=np.concatenate((high1, high2, high3, high4)))\n",
    "\n",
    "    # transformer l'observation récupérer par anylogic sous forme d'un vecteur de type Space de gym\n",
    "    def _convert_from_observation(self, observation: Observation):\n",
    "        print(\"observation.etatequipement\")\n",
    "        print(observation.etatequipement)\n",
    "        print(\"observation.disponibleequipe\")\n",
    "        print(observation.disponibleequipe)\n",
    "        print(\"observation.emplequipe\")\n",
    "        print(observation.emplequipe)\n",
    "        print(\"observation.recompns\")\n",
    "        print(observation.recompns)\n",
    "        print(\"FIN\")\n",
    "        return np.array(np.concatenate((observation.etatequipement, observation.disponibleequipe, observation.emplequipe)))\n",
    "\n",
    "    # définie l'espace d'action sous forme d'un vecteur avec les valeurs min et max de chaque action\n",
    "    def _get_action_space(self) -> spaces.Space:\n",
    "        return spaces.Box(low=np.array([0, 0, 0]), high=np.array([50, 50, 50]), shape=(3,), dtype=np.int64)\n",
    "\n",
    "    # transformer l'action prise par l'actor sous forme d'un vecteur à une action de type Action de Alpyne pour\n",
    "    # la communiquer avec le serveur alpyne\n",
    "    def _convert_to_action(self, action: np.ndarray) -> Action:\n",
    "        if(action[0] == action[1]):\n",
    "            action[1] = 50\n",
    "        if(action[0] == action[2]):\n",
    "            action[2] = 50\n",
    "        if(action[1] == action[2]):\n",
    "            action[2] = 50\n",
    "        return Action(req1=int(action[0]), req2=int(action[1]), req3=int(action[2]))\n",
    "\n",
    "    # calcul de la récompense\n",
    "    def _calc_reward(self, observation: Observation) -> float:\n",
    "        return observation.recompns\n",
    "\n",
    "    # définie une état terminale de l'epsode hors un nombre limité de pas\n",
    "    def _terminal_alternative(self, observation: Observation) -> bool:\n",
    "        eq = np.array(observation.etatequipement)\n",
    "        return np.count_nonzero(eq) >= 49\n",
    "        # arbitrarily chosen small(ish) number\n",
    "\n",
    "\n",
    "#sim = client.create_reinforcement_learning(cfg)\n",
    "\n",
    "# utiliser notre implémentation de l'environnement\n",
    "env = EolienParc(sim)\n",
    "\n",
    "env._get_observation_space()\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "The provided model run should not have been started!",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mcount_nonzero(eq) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m49\u001b[39m\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;66;03m# arbitrarily chosen small(ish) number\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#sim = client.create_reinforcement_learning(cfg)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \n\u001b[1;32m     75\u001b[0m \u001b[38;5;66;03m# utiliser notre implémentation de l'environnement\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mEolienParc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m env\u001b[38;5;241m.\u001b[39m_get_observation_space()\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mEolienParc.__init__\u001b[0;34m(self, sim)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, sim: ModelRun):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msim\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Stock/IA/Code/Anylogic/RL_Anylogic/alpyne/client/abstract.py:184\u001b[0m, in \u001b[0;36mBaseAlpyneEnv.__init__\u001b[0;34m(self, sim)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# complain if the sim was already started\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sim\u001b[38;5;241m.\u001b[39mid:\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided model run should not have been started!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim \u001b[38;5;241m=\u001b[39m sim\u001b[38;5;241m.\u001b[39mrun()  \u001b[38;5;66;03m# submit new run\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim\u001b[38;5;241m.\u001b[39mwait_for_completion()  \u001b[38;5;66;03m# wait until start is finished setting up\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: The provided model run should not have been started!"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.13 64-bit (conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "interpreter": {
   "hash": "1196c54bf83d23baa2b94a171650cda2fb5924a7b3110c84ce52ab2c5ef53ca5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}